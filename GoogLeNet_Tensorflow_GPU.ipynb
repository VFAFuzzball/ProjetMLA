{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19921,
     "status": "ok",
     "timestamp": 1671710096876,
     "user": {
      "displayName": "Victor Daube",
      "userId": "03006392672859710655"
     },
     "user_tz": -60
    },
    "id": "CXt05UtlLUcz",
    "outputId": "b80b7df8-cfb6-453d-ae79-4f86d70cc1e4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-22 13:09:10.000065: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-22 13:09:11.030415: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-22 13:09:11.030510: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-22 13:09:11.030523: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import datasets, layers, models, losses, Model\n",
    "import time\n",
    "from PIL import Image\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28602,
     "status": "ok",
     "timestamp": 1671710179607,
     "user": {
      "displayName": "Victor Daube",
      "userId": "03006392672859710655"
     },
     "user_tz": -60
    },
    "id": "_BedjJ0CrZgW",
    "outputId": "11eddaf5-4b9b-4e9d-a56e-128411b77b9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replace tiny-imagenet-200/words.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    }
   ],
   "source": [
    "!wget http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
    "!unzip -qq 'tiny-imagenet-200.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 46411,
     "status": "ok",
     "timestamp": 1671710385810,
     "user": {
      "displayName": "Victor Daube",
      "userId": "03006392672859710655"
     },
     "user_tz": -60
    },
    "id": "A-SW6_q8rmsz",
    "outputId": "9bb0b6ed-6597-497c-c8da-6d42b2dd0a39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting loading data\n",
      "finished loading data, in 24.430713891983032 seconds\n",
      "train data shape:  (100000, 64, 64, 3)\n",
      "train label shape:  (100000, 200)\n",
      "test data shape:  (10000, 64, 64, 3)\n",
      "test_labels.shape:  (10000, 200)\n"
     ]
    }
   ],
   "source": [
    "path = 'tiny-imagenet-200/'\n",
    "\n",
    "def get_id_dictionary():\n",
    "    id_dict = {}\n",
    "    for i, line in enumerate(open( path + 'wnids.txt', 'r')):\n",
    "        id_dict[line.replace('\\n', '')] = i\n",
    "    return id_dict\n",
    "  \n",
    "def get_class_to_id_dict():\n",
    "    id_dict = get_id_dictionary()\n",
    "    all_classes = {}\n",
    "    result = {}\n",
    "    for i, line in enumerate(open( path + 'words.txt', 'r')):\n",
    "        n_id, word = line.split('\\t')[:2]\n",
    "        all_classes[n_id] = word\n",
    "    for key, value in id_dict.items():\n",
    "        result[value] = (key, all_classes[key])      \n",
    "    return result\n",
    "\n",
    "def get_data(id_dict):\n",
    "    print('starting loading data')\n",
    "    train_data, test_data = [], []\n",
    "    train_labels, test_labels = [], []\n",
    "    t = time.time()\n",
    "    for key, value in id_dict.items():\n",
    "        train_data += [np.asarray(Image.open( path + 'train/{}/images/{}_{}.JPEG'.format(key, key, str(i))).convert(\"RGB\")) for i in range(500)]\n",
    "        train_labels_ = np.array([[0]*200]*500)\n",
    "        train_labels_[:, value] = 1\n",
    "        train_labels += train_labels_.tolist()\n",
    "\n",
    "    for line in open( path + 'val/val_annotations.txt'):\n",
    "        img_name, class_id = line.split('\\t')[:2]\n",
    "        test_data.append(np.asarray(Image.open( path + 'val/images/{}'.format(img_name)).convert(\"RGB\")))\n",
    "        test_labels_ = np.array([[0]*200])\n",
    "        test_labels_[0, id_dict[class_id]] = 1\n",
    "        test_labels += test_labels_.tolist()\n",
    "\n",
    "    print('finished loading data, in {} seconds'.format(time.time() - t))\n",
    "\n",
    "    return np.array(train_data), np.array(train_labels), np.array(test_data), np.array(test_labels)\n",
    "    #return train_data, train_labels, test_data, test_labels\n",
    "  \n",
    "train_data, train_labels, test_data, test_labels = get_data(get_id_dictionary())\n",
    "\n",
    "print( \"train data shape: \",  train_data.shape )\n",
    "print( \"train label shape: \", train_labels.shape )\n",
    "print( \"test data shape: \",   test_data.shape )\n",
    "print( \"test_labels.shape: \", test_labels.shape )\n",
    "\n",
    "def shuffle_data(train_data, train_labels ):\n",
    "    size = len(train_data)\n",
    "    train_idx = np.arange(size)\n",
    "    np.random.shuffle(train_idx)\n",
    "\n",
    "    return train_data[train_idx], train_labels[train_idx]\n",
    "  \n",
    "train_data, train_labels = shuffle_data(train_data, train_labels)\n",
    "\n",
    "# The data, shuffled and split between train and test sets:\n",
    "X_train = train_data\n",
    "Y_train = train_labels\n",
    "X_test = test_data\n",
    "Y_test = test_labels\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# subtract mean and normalize\n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "X_train -= mean_image\n",
    "X_test -= mean_image\n",
    "X_train /= 128.\n",
    "X_test /= 128.\n",
    "\n",
    "x_train = X_train\n",
    "y_train = Y_train\n",
    "x_val = X_test\n",
    "y_val = Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "baxvTVSQLcC9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 0s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-22 13:09:51.599390: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-12-22 13:09:51.599465: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:163] no NVIDIA GPU device is present: /dev/nvidia0 does not exist\n",
      "2022-12-22 13:09:51.600426: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test)=tf.keras.datasets.mnist.load_data()\n",
    "x_train = tf.pad(x_train, [[0, 0], [2,2], [2,2]])/255\n",
    "x_test = tf.pad(x_test, [[0, 0], [2,2], [2,2]])/255\n",
    "x_train = tf.expand_dims(x_train, axis=3, name=None)\n",
    "x_test = tf.expand_dims(x_test, axis=3, name=None)\n",
    "x_train = tf.repeat(x_train, 3, axis=3)\n",
    "x_test = tf.repeat(x_test, 3, axis=3)\n",
    "x_val = x_train[-2000:,:,:]\n",
    "y_val = y_train[-2000:]\n",
    "x_train = x_train[:-2000,:,:]\n",
    "y_train = y_train[:-2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 554,
     "status": "ok",
     "timestamp": 1671710393077,
     "user": {
      "displayName": "Victor Daube",
      "userId": "03006392672859710655"
     },
     "user_tz": -60
    },
    "id": "6wKcYCdaLcFU"
   },
   "outputs": [],
   "source": [
    "def inception(x,\n",
    "              filters_1x1,\n",
    "              filters_3x3_reduce,\n",
    "              filters_3x3,\n",
    "              filters_5x5_reduce,\n",
    "              filters_5x5,\n",
    "              filters_pool):\n",
    "  path1 = layers.Conv2D(filters_1x1, (1, 1), padding='same', activation='relu')(x)\n",
    "\n",
    "  path2 = layers.Conv2D(filters_3x3_reduce, (1, 1), padding='same', activation='relu')(x)\n",
    "  path2 = layers.Conv2D(filters_3x3, (1, 1), padding='same', activation='relu')(path2)\n",
    "\n",
    "  path3 = layers.Conv2D(filters_5x5_reduce, (1, 1), padding='same', activation='relu')(x)\n",
    "  path3 = layers.Conv2D(filters_5x5, (1, 1), padding='same', activation='relu')(path3)\n",
    "\n",
    "  path4 = layers.MaxPool2D((3, 3), strides=(1, 1), padding='same')(x)\n",
    "  path4 = layers.Conv2D(filters_pool, (1, 1), padding='same', activation='relu')(path4)\n",
    "\n",
    "  return tf.concat([path1, path2, path3, path4], axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 4219,
     "status": "ok",
     "timestamp": 1671710398268,
     "user": {
      "displayName": "Victor Daube",
      "userId": "03006392672859710655"
     },
     "user_tz": -60
    },
    "id": "nDEI1TMtLcHq"
   },
   "outputs": [],
   "source": [
    "inp = layers.Input(shape=(64, 64, 3))\n",
    "input_tensor = layers.experimental.preprocessing.Resizing(224, 224, interpolation=\"bilinear\", input_shape=x_train.shape[1:])(inp)\n",
    "\n",
    "x = layers.Conv2D(64, 7, strides=2, padding='same', activation='relu')(input_tensor)\n",
    "x = layers.MaxPooling2D(3, strides=2)(x)\n",
    "\n",
    "x = layers.Conv2D(64, 1, strides=1, padding='same', activation='relu')(x)\n",
    "x = layers.Conv2D(192, 3, strides=1, padding='same', activation='relu')(x)\n",
    "\n",
    "x = layers.MaxPooling2D(3, strides=2)(x)\n",
    "\n",
    "x = inception(x,\n",
    "              filters_1x1=64,\n",
    "              filters_3x3_reduce=96,\n",
    "              filters_3x3=128,\n",
    "              filters_5x5_reduce=16,\n",
    "              filters_5x5=32,\n",
    "              filters_pool=32)\n",
    "\n",
    "x = inception(x,\n",
    "              filters_1x1=128,\n",
    "              filters_3x3_reduce=128,\n",
    "              filters_3x3=192,\n",
    "              filters_5x5_reduce=32,\n",
    "              filters_5x5=96,\n",
    "              filters_pool=64)\n",
    "\n",
    "x = layers.MaxPooling2D(3, strides=2)(x)\n",
    "\n",
    "x = inception(x,\n",
    "              filters_1x1=192,\n",
    "              filters_3x3_reduce=96,\n",
    "              filters_3x3=208,\n",
    "              filters_5x5_reduce=16,\n",
    "              filters_5x5=48,\n",
    "              filters_pool=64)\n",
    "\n",
    "aux1 = layers.AveragePooling2D((5, 5), strides=3)(x)\n",
    "aux1 = layers.Conv2D(128, 1, padding='same', activation='relu')(aux1)\n",
    "aux1 = layers.Flatten()(aux1)\n",
    "aux1 = layers.Dense(1024, activation='relu')(aux1)\n",
    "aux1 = layers.Dropout(0.7)(aux1)\n",
    "aux1 = layers.Dense(10, activation='softmax')(aux1)\n",
    "\n",
    "x = inception(x,\n",
    "              filters_1x1=160,\n",
    "              filters_3x3_reduce=112,\n",
    "              filters_3x3=224,\n",
    "              filters_5x5_reduce=24,\n",
    "              filters_5x5=64,\n",
    "              filters_pool=64)\n",
    "\n",
    "x = inception(x,\n",
    "              filters_1x1=128,\n",
    "              filters_3x3_reduce=128,\n",
    "              filters_3x3=256,\n",
    "              filters_5x5_reduce=24,\n",
    "              filters_5x5=64,\n",
    "              filters_pool=64)\n",
    "\n",
    "x = inception(x,\n",
    "              filters_1x1=112,\n",
    "              filters_3x3_reduce=144,\n",
    "              filters_3x3=288,\n",
    "              filters_5x5_reduce=32,\n",
    "              filters_5x5=64,\n",
    "              filters_pool=64)\n",
    "\n",
    "aux2 = layers.AveragePooling2D((5, 5), strides=3)(x)\n",
    "aux2 = layers.Conv2D(128, 1, padding='same', activation='relu')(aux2)\n",
    "aux2 = layers.Flatten()(aux2)\n",
    "aux2 = layers.Dense(1024, activation='relu')(aux2)\n",
    "aux2 = layers.Dropout(0.7)(aux2)\n",
    "aux2 = layers.Dense(10, activation='softmax')(aux2)\n",
    "\n",
    "x = inception(x,\n",
    "              filters_1x1=256,\n",
    "              filters_3x3_reduce=160,\n",
    "              filters_3x3=320,\n",
    "              filters_5x5_reduce=32,\n",
    "              filters_5x5=128,\n",
    "              filters_pool=128)\n",
    "\n",
    "x = layers.MaxPooling2D(3, strides=2)(x)\n",
    "\n",
    "x = inception(x,\n",
    "              filters_1x1=256,\n",
    "              filters_3x3_reduce=160,\n",
    "              filters_3x3=320,\n",
    "              filters_5x5_reduce=32,\n",
    "              filters_5x5=128,\n",
    "              filters_pool=128)\n",
    "\n",
    "x = inception(x,\n",
    "              filters_1x1=384,\n",
    "              filters_3x3_reduce=192,\n",
    "              filters_3x3=384,\n",
    "              filters_5x5_reduce=48,\n",
    "              filters_5x5=128,\n",
    "              filters_pool=128)\n",
    "\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "x = layers.Dropout(0.4)(x)\n",
    "out = layers.Dense(200, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 548,
     "status": "ok",
     "timestamp": 1671710402160,
     "user": {
      "displayName": "Victor Daube",
      "userId": "03006392672859710655"
     },
     "user_tz": -60
    },
    "id": "WzIFlLP9LcLQ"
   },
   "outputs": [],
   "source": [
    "model = Model(inputs = inp, outputs = [out, aux1, aux2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 536,
     "status": "ok",
     "timestamp": 1671710441472,
     "user": {
      "displayName": "Victor Daube",
      "userId": "03006392672859710655"
     },
     "user_tz": -60
    },
    "id": "wJjdIxC1aWwO"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=[losses.categorical_crossentropy, losses.categorical_crossentropy, losses.categorical_crossentropy], loss_weights=[1, 0.3, 0.3], metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "VofnKKKoauYX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model\" is incompatible with the layer: expected shape=(None, 64, 64, 3), found shape=(None, 32, 32, 3)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileirsr85pm.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model\" is incompatible with the layer: expected shape=(None, 64, 64, 3), found shape=(None, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, [y_train, y_train, y_train], validation_data=(x_val, [y_val, y_val, y_val]), batch_size=64, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1671710219449,
     "user": {
      "displayName": "Victor Daube",
      "userId": "03006392672859710655"
     },
     "user_tz": -60
    },
    "id": "53UluuOE5TX_"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(15,15))\n",
    "\n",
    "axs[0].plot(history.history['loss'])\n",
    "axs[0].plot(history.history['val_loss'])\n",
    "axs[0].title.set_text('Training Loss vs Validation Loss')\n",
    "axs[0].set_xlabel('Epochs')\n",
    "axs[0].set_ylabel('Loss')\n",
    "axs[0].legend(['Train','Val'])\n",
    "\n",
    "axs[1].plot(history.history['dense_4_accuracy'])\n",
    "axs[1].plot(history.history['val_dense_4_accuracy'])\n",
    "axs[1].title.set_text('Training Accuracy vs Validation Accuracy')\n",
    "axs[1].set_xlabel('Epochs')\n",
    "axs[1].set_ylabel('Accuracy')\n",
    "axs[1].legend(['Train', 'Val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "aborted",
     "timestamp": 1671710219450,
     "user": {
      "displayName": "Victor Daube",
      "userId": "03006392672859710655"
     },
     "user_tz": -60
    },
    "id": "iNbFiTZW5dQ7"
   },
   "outputs": [],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "https://gist.github.com/mrgrhn/a6ad98dbc81f3ec8f73d39415452de9a#file-googlenet_tensorflow-ipynb",
     "timestamp": 1671528163229
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
